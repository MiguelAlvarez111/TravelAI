"""
FastAPI Backend para ViajeIA - Integraci√≥n con Google Gemini, OpenWeatherMap y Unsplash
"""
import os
import logging
import asyncio
import json
from typing import Optional, List, Dict
from collections import Counter
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

from services.gemini_service import get_gemini_service
from services.weather_service import get_weather_service
from services.unsplash_service import get_unsplash_service

# Cargar variables de entorno
load_dotenv()

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Sistema de m√©tricas simple (en memoria - se puede persistir en archivo si es necesario)
STATS_FILE = "stats.json"
stats = {
    "total_plans_generated": 0,
    "destinations_counter": {},
    "last_reset": datetime.now().isoformat()
}

# Cargar stats desde archivo si existe
def load_stats():
    """Carga estad√≠sticas desde archivo si existe."""
    global stats
    try:
        if os.path.exists(STATS_FILE):
            with open(STATS_FILE, 'r') as f:
                loaded_stats = json.load(f)
                stats.update(loaded_stats)
            logger.info(f"üìä Estad√≠sticas cargadas: {stats['total_plans_generated']} planes generados")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  No se pudo cargar stats.json: {e}. Iniciando con valores por defecto.")

def save_stats():
    """Guarda estad√≠sticas en archivo."""
    try:
        with open(STATS_FILE, 'w') as f:
            json.dump(stats, f, indent=2)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  No se pudo guardar stats.json: {e}")

def increment_plan_counter(destination: str):
    """Incrementa el contador de planes y actualiza el ranking de destinos."""
    global stats
    stats["total_plans_generated"] += 1
    if destination:
        destination_lower = destination.strip().lower()
        stats["destinations_counter"][destination_lower] = stats["destinations_counter"].get(destination_lower, 0) + 1
    save_stats()

# Cargar stats al iniciar
load_stats()

# Validar API KEY al iniciar
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    logger.warning(
        "‚ö†Ô∏è  ADVERTENCIA: GEMINI_API_KEY no encontrada en variables de entorno. "
        "El servidor puede fallar al procesar solicitudes. "
        "Aseg√∫rate de crear un archivo .env con tu API key."
    )
else:
    logger.info("‚úÖ GEMINI_API_KEY encontrada y validada")

# Inicializar FastAPI
app = FastAPI(
    title="ViajeIA API",
    description="API para recomendaciones de viaje con Google Gemini",
    version="1.0.0"
)

# Configurar CORS para permitir requests del frontend React
# En producci√≥n, permite todos los or√≠genes o lee desde variable de entorno
FRONTEND_URL = os.getenv("FRONTEND_URL", "*")
allowed_origins = ["*"] if FRONTEND_URL == "*" else [FRONTEND_URL, "http://localhost:3000", "http://localhost:5173"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,  # Permite todos en producci√≥n o espec√≠ficos seg√∫n configuraci√≥n
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Modelos Pydantic para validaci√≥n de requests
class TravelRequest(BaseModel):
    """Modelo para la solicitud de viaje del usuario con campos estructurados."""
    destination: str
    date: str
    budget: str
    style: str


class TravelResponse(BaseModel):
    """Modelo para la respuesta de recomendaci√≥n de viaje con datos en tiempo real."""
    gemini_response: str
    weather: Optional[Dict] = None
    images: List[str] = []
    info: Optional[Dict] = None


# Modelos para Chat con Memoria Conversacional
class ChatMessage(BaseModel):
    """Modelo para un mensaje individual en el historial de chat."""
    role: str  # "user" o "model"
    parts: str  # Contenido del mensaje


class ChatRequest(BaseModel):
    """Modelo para solicitud de chat con historial de conversaci√≥n."""
    destination: str
    date: str
    budget: str
    style: str
    message: str  # Nuevo mensaje del usuario
    history: List[ChatMessage] = []  # Historial de mensajes anteriores


class ChatResponse(BaseModel):
    """Modelo para respuesta de chat con memoria."""
    gemini_response: str
    weather: Optional[Dict] = None
    images: List[str] = []
    info: Optional[Dict] = None


@app.get("/")
async def root():
    """Endpoint ra√≠z para verificar que el servidor est√° funcionando."""
    return {
        "message": "üöÄ ViajeIA API est√° funcionando correctamente",
        "status": "ok",
        "endpoints": {
            "plan": "/api/plan",
            "chat": "/api/chat",
            "health": "/health"
        }
    }


@app.get("/health")
async def health_check():
    """Endpoint de health check."""
    try:
        # Intentar inicializar el servicio para verificar que todo est√° bien
        service = get_gemini_service()
        return {
            "status": "healthy",
            "gemini_service": "available"
        }
    except Exception as e:
        logger.error(f"Health check fall√≥: {e}")
        return {
            "status": "unhealthy",
            "gemini_service": "unavailable",
            "error": str(e)
        }


@app.get("/api/stats")
async def get_stats():
    """
    Endpoint para obtener estad√≠sticas de uso de la API.
    
    Returns:
        Dict con:
        - total_plans_generated: N√∫mero total de planes generados
        - top_destinations: Lista de los destinos m√°s populares
    """
    try:
        # Obtener top 5 destinos
        destinations_counter = Counter(stats["destinations_counter"])
        top_destinations = [
            {"destination": dest.capitalize(), "count": count}
            for dest, count in destinations_counter.most_common(5)
        ]
        
        return {
            "total_plans_generated": stats["total_plans_generated"],
            "top_destinations": top_destinations,
            "last_reset": stats.get("last_reset", "N/A")
        }
    except Exception as e:
        logger.error(f"‚ùå Error al obtener estad√≠sticas: {e}")
        raise HTTPException(
            status_code=500,
            detail="Error al obtener estad√≠sticas"
        )


@app.post("/api/plan")
async def create_travel_plan(request: TravelRequest):
    """
    Endpoint principal para generar recomendaciones de viaje con datos en tiempo real.
    
    Recibe una solicitud del usuario y consulta en paralelo:
    - Gemini para la recomendaci√≥n de viaje
    - OpenWeatherMap para el clima actual
    - Unsplash para im√°genes del destino
    
    Args:
        request: TravelRequest con la query del usuario y preferencias opcionales
        
    Returns:
        TravelResponse con la recomendaci√≥n de Gemini, clima, im√°genes e informaci√≥n adicional
        
    Raises:
        HTTPException: Si hay un error al procesar la solicitud
    """
    try:
        logger.info(f"üì® Nueva solicitud recibida: Destino={request.destination}, Fecha={request.date}, Presupuesto={request.budget}, Estilo={request.style}")
        
        # Validar que el destino no est√© vac√≠o
        if not request.destination or not request.destination.strip():
            raise HTTPException(
                status_code=400,
                detail="El destino no puede estar vac√≠o. Por favor, proporciona un destino para tu viaje."
            )
        
        destination = request.destination.strip()
        
        # Obtener servicios
        gemini_service = get_gemini_service()
        weather_service = get_weather_service()
        unsplash_service = get_unsplash_service()
        
        # Ejecutar llamadas en paralelo para mejor rendimiento
        logger.info("üîÑ Consultando Gemini, Weather y Unsplash en paralelo...")
        
        # Llamar a Gemini (s√≠ncrono pero lo ejecutamos en un executor para no bloquear)
        loop = asyncio.get_event_loop()
        gemini_task = loop.run_in_executor(
            None,
            lambda: gemini_service.generate_travel_recommendation(
                destination=destination,
                date=request.date,
                budget=request.budget,
                style=request.style
            )
        )
        
        # Llamadas as√≠ncronas a Weather y Unsplash
        weather_task = weather_service.get_weather(destination)
        images_task = unsplash_service.get_destination_images(destination, count=3)
        
        # Esperar todas las respuestas en paralelo
        gemini_response, weather_data, images = await asyncio.gather(
            gemini_task,
            weather_task,
            images_task,
            return_exceptions=True
        )
        
        # Manejar errores individuales sin fallar toda la respuesta
        if isinstance(gemini_response, Exception):
            logger.error(f"‚ùå Error en Gemini: {gemini_response}")
            raise HTTPException(
                status_code=500,
                detail="Ocurri√≥ un error consultando a la IA"
            )
        
        if isinstance(weather_data, Exception):
            logger.warning(f"‚ö†Ô∏è  Error al obtener clima: {weather_data}")
            weather_data = None
        
        if isinstance(images, Exception):
            logger.warning(f"‚ö†Ô∏è  Error al obtener im√°genes: {images}")
            images = []
        
        # Construir objeto info con datos adicionales
        info = {}
        if weather_data:
            info["local_time"] = weather_data.get("local_time", "N/A")
            # Opcional: agregar currency si se implementa en el futuro
        
        logger.info("‚úÖ Recomendaci√≥n generada con datos en tiempo real")
        
        # Incrementar contador de m√©tricas
        increment_plan_counter(destination)
        
        # Devolver respuesta con nueva estructura
        return {
            "gemini_response": gemini_response,
            "weather": {
                "temp": weather_data.get("temp") if weather_data else None,
                "condition": weather_data.get("condition") if weather_data else None,
                "feels_like": weather_data.get("feels_like") if weather_data else None
            } if weather_data else None,
            "images": images,
            "info": info if info else None
        }
        
    except HTTPException:
        # Re-lanzar HTTPExceptions sin modificar
        raise
    
    except ValueError as e:
        # Error de configuraci√≥n (API key faltante, etc.)
        logger.error(f"‚ùå Error de configuraci√≥n: {e}")
        raise HTTPException(
            status_code=500,
            detail="Error de configuraci√≥n del servidor. Por favor, contacta al administrador."
        )
    
    except Exception as e:
        # Bloque try/except simple: Si algo falla, devuelve un mensaje amigable
        error_message = str(e)
        logger.error(f"‚ùå Error al generar recomendaci√≥n: {error_message}")
        
        # Mensaje gen√©rico para errores
        raise HTTPException(
            status_code=500,
            detail="Ocurri√≥ un error consultando a la IA"
        )


@app.post("/api/chat")
async def chat_with_memory(request: ChatRequest):
    """
    Endpoint para chat continuo con memoria conversacional.
    
    Recibe un nuevo mensaje del usuario junto con el historial de conversaci√≥n
    y genera una respuesta contextualizada usando Gemini.
    
    Args:
        request: ChatRequest con el nuevo mensaje y el historial de conversaci√≥n
        
    Returns:
        ChatResponse con la respuesta de Gemini, clima, im√°genes e informaci√≥n adicional
        
    Raises:
        HTTPException: Si hay un error al procesar la solicitud
    """
    try:
        logger.info(f"üí¨ Nueva solicitud de chat: Destino={request.destination}, Mensaje={request.message[:50]}...")
        
        # Validar que el destino y mensaje no est√©n vac√≠os
        if not request.destination or not request.destination.strip():
            raise HTTPException(
                status_code=400,
                detail="El destino no puede estar vac√≠o."
            )
        
        if not request.message or not request.message.strip():
            raise HTTPException(
                status_code=400,
                detail="El mensaje no puede estar vac√≠o."
            )
        
        destination = request.destination.strip()
        message = request.message.strip()
        
        # Limitar el historial a los √∫ltimos 6 mensajes para optimizar tokens
        limited_history = request.history[-6:] if len(request.history) > 6 else request.history
        logger.info(f"üìö Historial limitado a {len(limited_history)} mensajes (de {len(request.history)} totales)")
        
        # Convertir objetos ChatMessage (Pydantic) a diccionarios para gemini_service
        # gemini_service espera List[Dict] y usa .get() en los mensajes
        # Compatible con Pydantic v1 (.dict()) y v2 (.model_dump())
        history_dicts = []
        for msg in limited_history:
            if isinstance(msg, dict):
                # Ya es un diccionario
                history_dicts.append(msg)
            elif hasattr(msg, 'model_dump'):
                # Pydantic v2
                history_dicts.append(msg.model_dump())
            elif hasattr(msg, 'dict'):
                # Pydantic v1
                history_dicts.append(msg.dict())
            else:
                # Fallback: convertir manualmente si es un objeto con atributos
                logger.warning(f"‚ö†Ô∏è  Formato de mensaje inesperado: {type(msg)}, convirtiendo manualmente")
                history_dicts.append({
                    "role": getattr(msg, 'role', 'user'),
                    "parts": getattr(msg, 'parts', '')
                })
        
        # Obtener servicios
        gemini_service = get_gemini_service()
        weather_service = get_weather_service()
        unsplash_service = get_unsplash_service()
        
        # Ejecutar llamadas en paralelo
        logger.info("üîÑ Consultando Gemini (con memoria), Weather y Unsplash en paralelo...")
        
        # Llamar a Gemini con historial
        loop = asyncio.get_event_loop()
        gemini_task = loop.run_in_executor(
            None,
            lambda: gemini_service.generate_chat_response(
                destination=destination,
                date=request.date,
                budget=request.budget,
                style=request.style,
                message=message,
                history=history_dicts
            )
        )
        
        # Llamadas as√≠ncronas a Weather y Unsplash
        weather_task = weather_service.get_weather(destination)
        images_task = unsplash_service.get_destination_images(destination, count=3)
        
        # Esperar todas las respuestas en paralelo
        gemini_response, weather_data, images = await asyncio.gather(
            gemini_task,
            weather_task,
            images_task,
            return_exceptions=True
        )
        
        # Manejar errores individuales
        if isinstance(gemini_response, Exception):
            logger.error(f"‚ùå Error en Gemini: {gemini_response}")
            raise HTTPException(
                status_code=500,
                detail="Ocurri√≥ un error consultando a la IA"
            )
        
        if isinstance(weather_data, Exception):
            logger.warning(f"‚ö†Ô∏è  Error al obtener clima: {weather_data}")
            weather_data = None
        
        if isinstance(images, Exception):
            logger.warning(f"‚ö†Ô∏è  Error al obtener im√°genes: {images}")
            images = []
        
        # Construir objeto info con datos adicionales
        info = {}
        if weather_data:
            info["local_time"] = weather_data.get("local_time", "N/A")
        
        logger.info("‚úÖ Respuesta de chat generada con memoria conversacional")
        
        # Devolver respuesta
        return {
            "gemini_response": gemini_response,
            "weather": {
                "temp": weather_data.get("temp") if weather_data else None,
                "condition": weather_data.get("condition") if weather_data else None,
                "feels_like": weather_data.get("feels_like") if weather_data else None
            } if weather_data else None,
            "images": images,
            "info": info if info else None
        }
        
    except HTTPException:
        raise
    
    except ValueError as e:
        logger.error(f"‚ùå Error de configuraci√≥n: {e}")
        raise HTTPException(
            status_code=500,
            detail="Error de configuraci√≥n del servidor. Por favor, contacta al administrador."
        )
    
    except Exception as e:
        error_message = str(e)
        logger.error(f"‚ùå Error al generar respuesta de chat: {error_message}")
        raise HTTPException(
            status_code=500,
            detail="Ocurri√≥ un error consultando a la IA"
        )


if __name__ == "__main__":
    import uvicorn
    
    logger.info("üöÄ Iniciando servidor ViajeIA...")
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,  # Auto-reload en desarrollo
        log_level="info"
    )

